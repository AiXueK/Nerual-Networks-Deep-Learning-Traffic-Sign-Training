{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from\n",
    "# http://christianherta.de/lehre/dataScience/machineLearning/neuralNetworks/reberGrammar.php\n",
    "\n",
    "# assign a number to each transition\n",
    "chars='BTSXPVE'\n",
    "\n",
    "# finite state machine for non-embedded Reber Grammar\n",
    "graph = [[(1,5),('T','P')] , [(1,2),('S','X')], \\\n",
    "         [(3,5),('S','X')], [(6,),('E')], \\\n",
    "         [(3,2),('V','P')], [(4,5),('V','T')] ]\n",
    "\n",
    "def get_one_example(min_length = 5):\n",
    "    seq = [0]\n",
    "    node = 0\n",
    "    prob = []\n",
    "    while node != 6:\n",
    "        this_prob = np.zeros(7)\n",
    "        transitions = graph[node]\n",
    "        if (len(seq) < min_length - 2) and (node == 2 or node == 4):\n",
    "            # choose transition to force a longer sequence\n",
    "            i = 1\n",
    "            this_prob[chars.find(transitions[1][1])] = 1 \n",
    "        else:\n",
    "            # choose transition randomly\n",
    "            i = np.random.randint(0, len(transitions[0]))\n",
    "            for ch in transitions[1]:\n",
    "                this_prob[chars.find(ch)] = 1./len(transitions[1])\n",
    "        prob.append(this_prob)\n",
    "        seq.append(chars.find(transitions[1][i]))\n",
    "        node = transitions[0][i]\n",
    "    return seq, prob\n",
    "\n",
    "def get_one_embedded_example(min_length=9):\n",
    "    i = np.random.randint(0,2)  # choose between 'T' and 'P'\n",
    "    if i == 0:\n",
    "        first = 1 # 'T'\n",
    "        prob1 = 1\n",
    "        prob4 = 0\n",
    "    else:\n",
    "        first = 4 # 'P'\n",
    "        prob1 = 0\n",
    "        prob4 = 1\n",
    "    seq_mid, prob_mid = get_one_example(min_length-4)\n",
    "    seq = [0,first] + seq_mid  + [first,6]\n",
    "    prob = [(0,0.5,0,0,0.5,0,0),(1,0,0,0,0,0,0)] + prob_mid + \\\n",
    "           [(0,prob1,0,0,prob4,0,0),(0,0,0,0,0,0,1)]\n",
    "    return seq, prob\n",
    "\n",
    "def get_reber_sequence(embedded=False,min_length=4):\n",
    "    if embedded:\n",
    "        seq, prob = get_one_embedded_example(min_length)\n",
    "    else:\n",
    "        seq, prob = get_one_example(min_length)\n",
    "\n",
    "    # convert numpy array to torch tensor\n",
    "    seq_torch = torch.from_numpy(np.asarray(seq))\n",
    "    input = F.one_hot(seq_torch[0:-1],num_classes=7).float()\n",
    "    label = seq_torch[1:]\n",
    "    probs = torch.from_numpy(np.asarray(prob)).float()\n",
    "    input = input.unsqueeze(0)\n",
    "    label = label.unsqueeze(0)\n",
    "    probs = probs.unsqueeze(0)\n",
    "    return input, label, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRN_model(nn.Module):\n",
    "    def __init__(self, num_input, num_hid, num_out, batch_size=1):\n",
    "        super().__init__()\n",
    "        self.num_hid = num_hid\n",
    "        self.batch_size = batch_size\n",
    "        self.W = nn.Parameter(torch.Tensor(num_input, num_hid))\n",
    "        self.U = nn.Parameter(torch.Tensor(num_hid, num_hid))\n",
    "        self.hid_bias = nn.Parameter(torch.Tensor(num_hid))\n",
    "        self.V = nn.Parameter(torch.Tensor(num_hid, num_out))\n",
    "        self.out_bias = nn.Parameter(torch.Tensor(num_out))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.num_hid)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return(torch.zeros(self.batch_size, self.num_hid))\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        batch_size, seq_size, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.num_hid).to(x.device)\n",
    "        else:\n",
    "            h_t = init_states\n",
    "         \n",
    "        for t in range(seq_size):\n",
    "            x_t = x[:, t, :]\n",
    "            c_t = x_t @ self.W + h_t @ self.U + self.hid_bias\n",
    "            h_t = torch.tanh(c_t)\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        # reshape from (sequence, batch, feature)\n",
    "        #           to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(0,1).contiguous()\n",
    "        output = hidden_seq @ self.V + self.out_bias\n",
    "        return output\n",
    "\n",
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self,num_input,num_hid,num_out,batch_size=1,num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_hid = num_hid\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.W = nn.Parameter(torch.Tensor(num_input, num_hid * 4))\n",
    "        self.U = nn.Parameter(torch.Tensor(num_hid, num_hid * 4))\n",
    "        self.hid_bias = nn.Parameter(torch.Tensor(num_hid * 4))\n",
    "        self.V = nn.Parameter(torch.Tensor(num_hid, num_out))\n",
    "        self.out_bias = nn.Parameter(torch.Tensor(num_out))\n",
    "        self.init_weights()\n",
    "                \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.num_hid)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return(torch.zeros(self.num_layers, self.batch_size, self.num_hid),\n",
    "               torch.zeros(self.num_layers, self.batch_size, self.num_hid))\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        batch_size, seq_size, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t, c_t = (torch.zeros(batch_size,self.num_hid).to(x.device), \n",
    "                        torch.zeros(batch_size,self.num_hid).to(x.device))\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "         \n",
    "        NH = self.num_hid\n",
    "        for t in range(seq_size):\n",
    "            x_t = x[:, t, :]\n",
    "            # batch the computations into a single matrix multiplication\n",
    "            gates = x_t @ self.W + h_t @ self.U + self.hid_bias\n",
    "            i_t, f_t, g_t, o_t = (\n",
    "                torch.sigmoid(gates[:, :NH]),     # input gate\n",
    "                torch.sigmoid(gates[:, NH:NH*2]), # forget gate\n",
    "                torch.tanh(gates[:, NH*2:NH*3]),  # new values\n",
    "                torch.sigmoid(gates[:, NH*3:]),   # output gate\n",
    "            )\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        # reshape from (sequence, batch, feature)\n",
    "        #           to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(0,1).contiguous()\n",
    "        output = hidden_seq @ self.V + self.out_bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_type='srn', hid=8, embed=False, length=4, lr=0.3):\n",
    "    if model_type == 'srn':\n",
    "        model = SRN_model(7,hid,7)\n",
    "    elif model_type == 'lstm':\n",
    "        model = LSTM_model(7,hid,7)\n",
    "\n",
    "    loss_function = F.nll_loss\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    np.set_printoptions(suppress=True,precision=2)\n",
    "\n",
    "    for epoch in range(50001):\n",
    "        model.zero_grad()\n",
    "        input, label, prob = get_reber_sequence(embedded=embed,\n",
    "                                                min_length=length)\n",
    "        model.init_hidden()\n",
    "        output = model(input)\n",
    "        log_prob  = F.log_softmax(output, dim=2)\n",
    "        loss = loss_function(log_prob.squeeze(), label.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            # Check accuracy during training\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                input, label, prob = get_reber_sequence(embedded=embed,\n",
    "                                                        min_length=length)\n",
    "                model.init_hidden()\n",
    "                output = model(input)\n",
    "                log_prob  = F.log_softmax(output, dim=2)\n",
    "                prob_out = torch.exp(log_prob)\n",
    "                print('-----')\n",
    "                symbol = [chars[index] for index in label.squeeze().tolist()]\n",
    "                print('symbol = B'+''.join(symbol))\n",
    "                print('label =',label.squeeze().numpy())\n",
    "                print('true probabilities:')\n",
    "                print(prob.squeeze().numpy())\n",
    "                print('output probabilities:')\n",
    "                print(prob_out.squeeze().numpy())\n",
    "                print('epoch: %d' %epoch)\n",
    "                if embed:\n",
    "                    prob_out_mid   = prob_out[:,2:-3,:]\n",
    "                    prob_out_final = prob_out[:,-2,:]\n",
    "                    prob_mid   = prob[:,2:-3,:]\n",
    "                    prob_final = prob[:,-2,:]\n",
    "                    print('error: %1.4f' %torch.mean((prob_out_mid - prob_mid)\n",
    "                                                    *(prob_out_mid - prob_mid)))\n",
    "                    print('final: %1.4f' %torch.mean((prob_out_final - prob_final)\n",
    "                                                    *(prob_out_final - prob_final)))\n",
    "                else:\n",
    "                    print('error: %1.4f' %torch.mean((prob_out - prob)\n",
    "                                                    *(prob_out - prob)))\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='srn'\n",
    "hid=8\n",
    "embed=True\n",
    "length=4\n",
    "\n",
    "train(model_type=model, hid=hid, embed=embed, length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='lstm'\n",
    "hid=8\n",
    "embed=True\n",
    "length=4\n",
    "\n",
    "train(model_type=model, hid=hid, embed=embed, length=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='lstm'\n",
    "hid=16\n",
    "embed=True\n",
    "length=12\n",
    "\n",
    "train(model_type=model, hid=hid, embed=embed, length=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
